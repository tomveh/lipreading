#!/bin/bash
#SBATCH --time=05-00
#SBATCH --gres=gpu:v100:1
#SBATCH --cpus-per-task=12
#SBATCH --mem-per-cpu=4G

source activate lipreading

mkdir /tmp/$SLURM_JOB_ID
trap "rm -rf /tmp/$SLURM_JOB_ID; exit" TERM EXIT

cd /tmp/$SLURM_JOB_ID
mkdir lrs2
cd lrs2
mkdir mvlrs_v1
cp /scratch/elec/puhe/c/LRS3-TED/lrs2/*.txt .
cp /scratch/elec/puhe/c/LRS3-TED/lrs2/pretrain_features.tar .
tar xf pretrain_features.tar
rm pretrain_features.tar
mv pretrain_features mvlrs_v1
cd /scratch/work/vehvilt2/lipreading

srun python src/train.py\
     --learning_rate 1e-4\
     --weight_decay 1e-4\
     --batch_size 128\
     --data_root "/tmp/$SLURM_JOB_ID/"\
     --workers 12\
     --valid_interval 50\
     --gpus 1\
     --min_epochs 1000000\
     --max_epochs 1000000\
     --description "load weights from 52074962 and start with seq len 3"\
     --seq_inc_interval 1000\
     --model_weights "tb_logs/train/version_52074962_lr=0.0001_bs=128_gpus=1_description=run-train-with-pretrained-lrw1-weights_date=Apr-22-11:19/epoch=3800-val_loss=1.09.ckpt"

