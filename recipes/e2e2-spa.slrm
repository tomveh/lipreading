#!/bin/bash -l
#SBATCH -p dgx-spa
#SBATCH --time=05-00
#SBATCH --gres=gpu:v100:1
#SBATCH --cpus-per-task=10
#SBATCH --mem-per-cpu=4G
#SBATCH --export=HOME,USER,TEAM,WRKDIR

source activate lipreading

# mkdir /tmp/$SLURM_JOB_ID
# trap "rm -rf /tmp/$SLURM_JOB_ID; exit" TERM EXIT

# cd /tmp/$SLURM_JOB_ID
# mkdir lrs2
# cd lrs2
# cp /scratch/elec/puhe/c/LRS3-TED/lrs2/*.txt .
# cp /scratch/elec/puhe/c/LRS3-TED/lrs2/lrs2_v1.tar .
# tar xf lrs2_v1.tar
# rm lrs2_v1.tar
# cd ..

# mkdir lrs3
# cd lrs3
# cp /scratch/elec/puhe/c/LRS3-TED/lrs3/zip/*.zip .
# unzip -q lrs3_pretrain.zip
# unzip -q lrs3_trainval.zip
# rm *.zip

cd /scratch/work/vehvilt2/lipreading

python src/e2e2.py\
     --learning_rate 1e-5\
     --weight_decay 1e-4\
     --batch_size 64\
     --data_root "/scratch/elec/puhe/c/LRS3-TED/"\
     --workers 10\
     --gpus 1\
     --accumulate_grad_batches 1\
     --max_epochs 100000\
     --description "continue 52653259 with seq len 4 (lr callback)"\
     --model_weights "/scratch/work/vehvilt2/lipreading/tb_logs/e2e_subword/version_52653259_lr=5e-05_bs=64_gpus=1_description=continue-52453966-with-seq-len-3-using-linear-warmup-lr-scheduler----use-load_from_checkpoint---limit-lr-to-1e-5-and-warmup-over-10-epochs_date=May-14-21:33"
